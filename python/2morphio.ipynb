{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python and Morph.io (for scraping)\n",
    "\n",
    "This tutorial is intended to build on some basic coding concepts and introduce Morph.io. By the end you should:\n",
    "\n",
    "* Be able to use GitHub to edit Python files\n",
    "* Use Morph.io to run Python code hosted on GitHub\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "\n",
    "## Get started with Morph.io\n",
    "\n",
    "1. [create an account on GitHub](https://github.com/) if you haven't got one already, and [sign in to Morph.io using your GitHub account](https://morph.io/users/auth/github)\n",
    "2. [Click **New scraper**](https://morph.io/scrapers/new) on the menu at the top of Morph.io. You will be taken to a new page asking you to specify more details\n",
    "3. On the dropdown menu for *Language*, select **Python**. Give your scraper a name in the next box - something like 'startingtocode' (no spaces), and in the final box write 'none' - this isn't a scraper yet: we're just using Morph.io as a place to learn code.\n",
    "4. Click **Create scraper**\n",
    "\n",
    "It will take Morph.io a few moments to create the files for your scraper (the files are being created on GitHub). \n",
    "\n",
    "When it has finished, you will be taken to a new page for the scraper. Look on the right where it says *Scraper code*. There should be a link to `startingtocode / scraper.py ` - this will take you to the pages on GitHub where the code is now hosted: `scraper.py` is the file for the code itself; `startingtocode` is the link to the repository containing that file.\n",
    "\n",
    "Open the link to `scraper.py` in a separate tab or window, but also keep your Morph.io page for this scraper open in another tab or window - you will need to edit the code on GitHub, and run it to see the results on Morph.io.\n",
    "\n",
    "Now we're ready to start.\n",
    "\n",
    "## Introducing the template code\n",
    "\n",
    "When you create a new scraper on Morph.io, it creates it with some template code as shown below. \n",
    "\n",
    "Each line begins with a hash symbol: `#`. There are two ways that these are most commonly used:\n",
    "\n",
    "* Firstly, as a way of creating **comments** in Python code: any code starting with a `#` does not do anything, so the hash symbol allows you to add comments which are not treated as working code.\n",
    "* Secondly, as a way of *disabling* code - what's called *commenting out* code. Rather than delete an entire line of code, it is easier to add a `#` at the front to turn it 'off' to test what happens, so you can always turn it back 'on' again quickly by removing the `#`.\n",
    "\n",
    "In the template code below generated by Morph.io, the *entire* code is commented out. The idea is that you can **uncomment** the sections you want to use in your own code, saving you time writing scraping code from scratch. We'll come back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "# import scraperwiki\n",
    "# import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "# html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "# root = lxml.html.fromstring(html)\n",
    "# root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries in Morph.io\n",
    "\n",
    "Make sure you are on this file in GitHub, and click the edit button to make some changes.\n",
    "\n",
    "Uncomment the two lines that start with `import` so the code looks like below.\n",
    "\n",
    "These two lines bring in two **libraries** to Morph.io: \n",
    "\n",
    "* Scraperwiki is a library which has useful functions for scraping webpages and storing the results in a database\n",
    "* lxml.html is a library which is useful for *parsing* HTML webpages - i.e. drilling down to particular pieces of information you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "# html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "# root = lxml.html.fromstring(html)\n",
    "# root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, uncomment the line `html = scraperwiki.scrape(\"http://foo.com\")`. \n",
    "\n",
    "This line is looking at a URL - foo.com - so it's worth checking that site in another window to see what's there.\n",
    "\n",
    "It's in parentheses, which means it's being used as an ingredient in a function - `scrape()`. Specifically, `scraperwiki.scrape()`, which means it's part of the **scraperwiki library**. \n",
    "\n",
    "When using a library it's always useful to check the **documentation** for that library - [here's the documentation for Scraperwiki](https://classic.scraperwiki.com/docs/python/), or at least it's 'Classic' version which was used by Morph.io. There's a link to [where the documentation is now hosted, on GitHub](https://github.com/scraperwiki/code-scraper-in-browser-tool/wiki)\n",
    "\n",
    "The `scrape` function grabs the contents of the given URL and stores it in the new variable `html`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "# root = lxml.html.fromstring(html)\n",
    "# root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **commit** your changes (GitHub's version of saving), and switch back to the scraper in Morph.io. Run the scraper.\n",
    "\n",
    "The first time you do this you may have to wait while Morph.io installs the libraries - but this only has to happen once and it should run more quickly the second time.\n",
    "\n",
    "*Note: if you get a 'status code 255' error then it means Morph.io isn't working properly right now. You can only leave it and come back later. (This doesn't happen that often).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section of code *converts* the `html` variable into another new variable called `root`, and drills down further into that using something called `cssselect`, which uses **css selectors** to grab very specific pieces of information from the page. We'll talk about this in class but search around for more about those selectors and think how they could be used in scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "root = lxml.html.fromstring(html)\n",
    "root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we see what's happening? \n",
    "\n",
    "Add a `print` command - or three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "root = lxml.html.fromstring(html)\n",
    "print html\n",
    "print root\n",
    "print root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first `print` command prints the variable `html` and the second `print` command prints the variable `root` - but the third doesn't print any variable at all. It is just added to the front of the line `root.cssselect(\"div[align='left']\")` - that's because the results of that line of code are not saved anywhere. \n",
    "\n",
    "We could save it in a variable first and then print it. How would you do that?\n",
    "\n",
    "Anyway, the results of those last two print commands are a bit cryptic. Here's one:\n",
    "\n",
    "`<Element html at 0x7faa5365c470>`\n",
    "\n",
    "And here's the other:\n",
    "\n",
    "`[]`\n",
    "\n",
    "The `<Element html at 0x7faa5365c470>` bit looks funny because `root` has been created with an `lxml.html` function: it what we call an lxml.html *object*. So what this tells us is that we need to find some way to convert or decode this information back into something understandable.\n",
    "\n",
    "In theory that's what `cssselect` should do. But the results of that are pretty unimpressive: `[]`\n",
    "\n",
    "What can we work out from that? The square brackets are a big clue. Square brackets indicate a **list**, and that's exactly what `cssselect` generates: a list of elements that match a css selector.\n",
    "\n",
    "But there's nothing in those square brackets, so our list is **empty**. Why? Because `cssselect` found *no* matches. \n",
    "\n",
    "Look at the code: `cssselect` was looking for this: `\"div[align='left']\"`. In other words, any content inside the HTML tags `<div align=\"left\">`.\n",
    "\n",
    "Check the source code of the webpage that is being scraped: foo.com. Are there any such tags? No. \n",
    "\n",
    "We can alter it, then, to look for a tag we *know* is on that page: link tags for example. The css selector for a link is `a` (as in `<a href=`), so we can alter the code like so:\n",
    "\n",
    "`root.cssselect(\"a\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "root = lxml.html.fromstring(html)\n",
    "print html\n",
    "print root\n",
    "print root.cssselect(\"a\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we get a different result:\n",
    "\n",
    "`[<Element a at 0x7f80ac4b4838>, <Element a at 0x7f80ac4b4890>]`\n",
    "\n",
    "Once again those square brackets tell us this is a list, and we can count two items in that list now. Checking the source HTML on the webpage we can see there are two `a` tags there too. But again these have been encoded as lxml objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
