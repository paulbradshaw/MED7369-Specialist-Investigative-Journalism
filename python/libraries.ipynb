{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "\n",
    "Python comes with a number of built-in functions, but you can access more functions by importing **libraries**. \n",
    "\n",
    "Libraries are collections of functions and other code that someone has created around a particular problem. It is one of the great features of modern programming: anyone can create and share their own code for others to use and build upon, as new problems come along. \n",
    "\n",
    "There are libraries for scraping, for dealing with particular data types such as JSON and XML, for data visualisation, for statistical analysis, for working with dates and times, for producing HTML and JavaScript outputs, and many other situations.\n",
    "\n",
    "You can import and install a library in a Jupyter notebook by using `!conda install` or `!pip install` followed by the name of the library. Some more specific [good practice is outlined in this post](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/): first you must `import sys` and then use `!conda install --yes --prefix {sys.prefix}` followed by the name of the package/library that you want to install.\n",
    "\n",
    "Here, for example, is the line to *install* the `pandas` library which is [used for data analysis](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#code taken from https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, the library needs to be *imported* to be activated and available to use in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions and methods from the library are typically prefixed by the name of the library and a period. You can spot a `pandas` function or method, then, because it begins `pandas.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04',\n",
      "               '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n",
      "              dtype='datetime64[ns]', freq='D')\n"
     ]
    }
   ],
   "source": [
    "#This code is adapted from https://pandas.pydata.org/pandas-docs/stable/basics.html\n",
    "#It uses the date_range function to create a series of dates, then assigns those to a variable called 'index'\n",
    "index = pandas.date_range('1/1/2000', periods=8)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you will sometimes see libraries installed and given a simpler alias, like `pd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the functions will be preceded by the alias (in this case `pd.`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04',\n",
      "               '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n",
      "              dtype='datetime64[ns]', freq='D')\n"
     ]
    }
   ],
   "source": [
    "#This code is adapted from https://pandas.pydata.org/pandas-docs/stable/basics.html\n",
    "#It uses the date_range function to create a series of dates, then assigns those to a variable called 'index'\n",
    "index = pd.date_range('1/1/2000', periods=8)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Scraperwiki library\n",
    "\n",
    "Here is the line to import the [scraperwiki library](https://classic.scraperwiki.com/docs/python/python_help_documentation/) which contains useful functions for downloading webpages, extracting information from those, and saving that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bfailed\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - scraperwiki\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.continuum.io/pkgs/main/osx-64\n",
      "  - https://repo.continuum.io/pkgs/main/noarch\n",
      "  - https://repo.continuum.io/pkgs/free/osx-64\n",
      "  - https://repo.continuum.io/pkgs/free/noarch\n",
      "  - https://repo.continuum.io/pkgs/r/osx-64\n",
      "  - https://repo.continuum.io/pkgs/r/noarch\n",
      "  - https://repo.continuum.io/pkgs/pro/osx-64\n",
      "  - https://repo.continuum.io/pkgs/pro/noarch\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes --prefix {sys.prefix} scraperwiki\n",
    "import scraperwiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once imported, we can follow instructions on [the documentation for that library](https://classic.scraperwiki.com/docs/python/python_help_documentation/) to use particular functions.\n",
    "\n",
    "### The lxml library\n",
    "\n",
    "Another library we need is `lxml.html` - first the `lxml` package is installed then the `lxml.html` part of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/paul/anaconda\n",
      "\n",
      "  added / updated specs: \n",
      "    - lxml\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    icu-58.2                   |       h4b95b61_1        22.3 MB\n",
      "    lxml-4.1.1                 |   py35hef8c89e_1         1.3 MB\n",
      "    gettext-0.19.8.1           |       h15daf44_3         3.4 MB\n",
      "    ncurses-6.0                |       hd04f020_2         842 KB\n",
      "    libiconv-1.15              |       hdd342a3_7         1.3 MB\n",
      "    expat-2.2.5                |       hb8e80ba_0         128 KB\n",
      "    libffi-3.2.1               |       h475c297_4          40 KB\n",
      "    pcre-8.41                  |       hfb6ab37_1         222 KB\n",
      "    libedit-3.1                |       hb4e282d_0         153 KB\n",
      "    python-3.5.4               |      he720263_23        14.9 MB\n",
      "    libxml2-2.9.7              |       hab757c2_0         1.9 MB\n",
      "    readline-7.0               |       hc1231fa_4         393 KB\n",
      "    sqlite-3.22.0              |       h3efe00b_0         1.9 MB\n",
      "    libxslt-1.1.32             |       hb819dd2_0         491 KB\n",
      "    glib-2.53.6                |       h33f6a65_2         4.8 MB\n",
      "    pyqt-5.6.0                 |   py35hbd126f6_6         4.2 MB\n",
      "    tk-8.6.7                   |       h35a86e2_3         3.2 MB\n",
      "    dbus-1.12.2                |       h5243cc1_1         538 KB\n",
      "    qt-5.6.2                   |      h9975529_14        65.3 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       127.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    dbus:     1.12.2-h5243cc1_1  \n",
      "    expat:    2.2.5-hb8e80ba_0   \n",
      "    gettext:  0.19.8.1-h15daf44_3\n",
      "    glib:     2.53.6-h33f6a65_2  \n",
      "    libedit:  3.1-hb4e282d_0     \n",
      "    libffi:   3.2.1-h475c297_4   \n",
      "    libiconv: 1.15-hdd342a3_7    \n",
      "    ncurses:  6.0-hd04f020_2     \n",
      "    pcre:     8.41-hfb6ab37_1    \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    icu:      54.1-0              --> 58.2-h4b95b61_1     \n",
      "    libxml2:  2.9.2-0             --> 2.9.7-hab757c2_0    \n",
      "    libxslt:  1.1.28-2            --> 1.1.32-hb819dd2_0   \n",
      "    lxml:     3.6.4-py35_0        --> 4.1.1-py35hef8c89e_1\n",
      "    pyqt:     5.6.0-py35_0        --> 5.6.0-py35hbd126f6_6\n",
      "    python:   3.5.2-0             --> 3.5.4-he720263_23   \n",
      "    qt:       5.6.0-0             --> 5.6.2-h9975529_14   \n",
      "    readline: 6.2-2               --> 7.0-hc1231fa_4      \n",
      "    sqlite:   3.13.0-0            --> 3.22.0-h3efe00b_0   \n",
      "    tk:       8.5.18-0            --> 8.6.7-h35a86e2_3    \n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "icu 58.2: ############################################################## | 100% \n",
      "lxml 4.1.1: ############################################################ | 100% \n",
      "gettext 0.19.8.1: ###################################################### | 100% \n",
      "ncurses 6.0: ########################################################### | 100% \n",
      "libiconv 1.15: ######################################################### | 100% \n",
      "expat 2.2.5: ########################################################### | 100% \n",
      "libffi 3.2.1: ########################################################## | 100% \n",
      "pcre 8.41: ############################################################# | 100% \n",
      "libedit 3.1: ########################################################### | 100% \n",
      "python 3.5.4: ########################################################## | 100% \n",
      "libxml2 2.9.7: ######################################################### | 100% \n",
      "readline 7.0: ########################################################## | 100% \n",
      "sqlite 3.22.0: ######################################################### | 100% \n",
      "libxslt 1.1.32: ######################################################## | 100% \n",
      "glib 2.53.6: ########################################################### | 100% \n",
      "pyqt 5.6.0: ############################################################ | 100% \n",
      "tk 8.6.7: ############################################################## | 100% \n",
      "dbus 1.12.2: ########################################################### | 100% \n",
      "qt 5.6.2: ############################################################## | 100% \n",
      "Preparing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
      "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
      "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes --prefix {sys.prefix} lxml\n",
    "import lxml.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `cssselect` library\n",
    "\n",
    "In the code below we install a further library - `cssselect` to drill down into a scraped page using css selectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element html at 0x1044cc598>\n",
      "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/paul/anaconda\n",
      "\n",
      "  added / updated specs: \n",
      "    - cssselect\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    cssselect-1.0.3            |           py35_0          28 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    cssselect: 1.0.3-py35_0\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "cssselect 1.0.3: ####################################################### | 100% \n",
      "Preparing transaction: / \b\bdone\n",
      "Verifying transaction: \\ \b\bdone\n",
      "Executing transaction: / \b\bdone\n"
     ]
    }
   ],
   "source": [
    "myurl = \"https://www.bbc.co.uk/\"\n",
    "html = scraperwiki.scrape(myurl)\n",
    "#print(html)\n",
    "#convert it to an lxml object\n",
    "root = lxml.html.fromstring(html)\n",
    "print(root)\n",
    "\n",
    "#cssselect is now its own library: https://cssselect.readthedocs.io/en/latest/\n",
    "!conda install --yes --prefix {sys.prefix} cssselect\n",
    "import cssselect\n",
    "#these lines generate an error - unable to find cssselect\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use that, as well as the other libraries, to drill into a scraped page and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FA Cup: Chelsea v Hull  and Leicester v Sheffield Utd. Live now.\n",
      "US election tampering charge for Russians\n",
      "Why this toothbrush debate is dividing the internet\n",
      "US election tampering charge for Russians\n",
      "FBI admits botching tip on Florida gunman\n",
      "Merkel 'curious' about UK's Brexit aims\n",
      "FA Cup: Chelsea v Hull and Leicester v Sheffield Utd. Live now.\n",
      "GB's Parsons wins skeleton bronze\n",
      "Willian curls in beautiful Chelsea opener\n",
      "Kiwi devastated after shock skeleton mistake\n",
      "Jamaica to compete after beer producer 'donates' bobsleigh\n",
      "Find out which countries are topping the medal table\n",
      "11 forgotten songs about football stars\n",
      "Does Marvel's Black Panther live up to the hype?\n",
      "Films about brainwashing we can't take our eyes off\n",
      "The Cardiff murder that sparked a scandal\n",
      "The man who got away with $242m using 'black magic'\n",
      "Couple surprise each other by proposing at the same time\n",
      "Allison Janney felt 'liberated' in I, Tonya role\n",
      "Tambor criticises Amazon sexual harassment investigation\n",
      "Jennifer Aniston and Justin Theroux announce split\n",
      "Should Oscar-bait films be banished to Room 101?\n",
      "JK Rowling's crime drama returns for new BBC One series\n",
      "China's biggest TV show in 'racist blackface' furore\n",
      "Collateral\n",
      "The Young Offenders\n",
      "Stacey Dooley Investigates\n",
      "Shetland\n",
      "Earth's Natural Wonders\n",
      "The Mash Report\n",
      "Seven reasons why you should give teenagers a break\n",
      "Why people are choosing to quit social media\n",
      "Celebrate Chinese New Year with these 12 easy dishes\n",
      "Seven reasons why you should give teenagers a break\n",
      "Why people are choosing to quit social media\n",
      "Celebrate Chinese New Year with these 12 easy dishes\n",
      "Top tips for surviving the half term holidays with your little one\n",
      "Five simple ways to make food fun for kids\n",
      "Six quick and easy hairstyles to try with your child\n",
      "Real ale magazine apologises for 'offensive' crossword\n",
      "Would you work for free to secure a job?\n",
      "Should £10,000 be given to everyone under 55?\n",
      "Has the city found an answer to the Brexit question?\n",
      "Golden eagle disappearance 'highly suspicious'\n",
      "Florida shooting: Who are the victims?\n",
      "West Brom players apologise after taxi incident\n",
      "Which teams will progress in the FA Cup fifth round?\n",
      "Williams unveil new Mercedes-inspired car for F1 season\n",
      "The ingenious secret of how hares hide in plain sight\n",
      "How does a racing driver with no legs compete?\n",
      "How is Chinese New Year celebrated?\n",
      "Is reading better for you than a spa?\n",
      "Judges choose Booker winners for 50th anniversary award\n",
      "Blade Runner: 'How we recreated a character'\n",
      "'How I escaped the infamous Westboro Baptist Church'\n",
      "Woman held for 'posing' as groom for dowry\n",
      "Lack of good RE ‘leaves pupils at risk’ of extremism\n",
      "Treat yourself to a slow-cooked curry this weekend\n",
      "Tom Kerridge's healthy sweet potato and black bean burritos\n",
      "Save on the washing up with these 12 one-pot wonders\n",
      "11 massive moments that shaped 2002\n",
      "The saddest real-life stories behind pop songs\n",
      "Seven big names in music set to return this year\n",
      "What's your Chinese zodiac animal?\n",
      "News quiz: How did this man get out of jail?\n",
      "Can you match the Champions League players to their teams?\n",
      "Things not to say to people from Essex\n",
      "The bobsleigh star you might recognise from TV game shows\n",
      "Eight times TV co-stars just couldn't get along\n",
      "The Bottom Line\n",
      "Sarah Kendall: Australian Trilogy\n",
      "Assignment\n",
      "Radio 3 in Concert\n",
      "Bob Harris Country\n",
      "Sherlock Holmes\n"
     ]
    }
   ],
   "source": [
    "h3s = root.cssselect('h3')\n",
    "#print(len(h3s))\n",
    "#print h3s[0].text_content()\n",
    "for h3 in h3s:\n",
    "    print(h3.text_content())\n",
    "record = {\"name\":\"paul\", \"age\": 25}\n",
    "scraperwiki.sqlite.save(['name'], record, table_name='somepeople')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening your Scraperwiki SQL database using command line\n",
    "\n",
    "When you use `scraperwiki.sqlite.save` the data is saved to a SQLite database in the same directory as your code. If you look in the same folder you should see a file called 'scraperwiki.sqlite'.\n",
    "\n",
    "One of the advantages of using Morph.io or Quickcode.io to run scraper code is that they have built-in functionality for seeing the data from that database and querying it. So that's generally going to be the best option.\n",
    "\n",
    "However, if you are doing it locally there are ways to query the database that is now stored on your machine.\n",
    "\n",
    "If you have a Mac (on which sqlite3 is pre-installed) you can do this through the command line:\n",
    "\n",
    "* Open Terminal and navigate to the folder containing your sqlite database (as outlined above, this should be the same location as your code). If you don't know how to do this, check the [guide to command line](https://github.com/paulbradshaw/commandline).\n",
    "* Type `sqlite3`. This starts SQLite and you should now see a `sqlite3>` prompt at the start of every line. Helpfully, it instructs you to *Use \".open FILENAME\" to reopen on a persistent database.* So do that:\n",
    "* Type `.open scraperwiki.sqlite`\n",
    "* Type `.schema` - this should show you the structure of the database which can help you to write a query. It will have a structure like: `CREATE TABLE somepeople ( name TEXT, age BIGINT );`. Any word after `CREATE TABLE` is the name of a table (in this case `somepeople`). Then in parenthesse you have the name of any fields in that table followed by the type of data that field holds: in this case there is a field called `name` containing data of the type `TEXT`, and a second field called `age` of the type `BIGINT` (integer).\n",
    "* Now you know what the tables and fields are called, you can write a query. For example: `SELECT * FROM somepeople;` - note that the query ends with a semi-colon. If you forget to add a semi-colon you will be prompted `...>` to continue writing your query. Just type `;` if this happens to end the query.\n",
    "* You should now see the results of your query (all the data in the table). You can refine it further by naming specific fields and/or criteria, e.g. `SELECT age FROM somepeople;` or `SELECT * FROM somepeople WHERE age > 20;`\n",
    "\n",
    "You can [find more about sqlite3 (including using it on Windows) here](https://sqlite.org/cli.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: pandas\n",
    "\n",
    "https://www.dataquest.io/blog/python-pandas-databases/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
